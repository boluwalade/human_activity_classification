---
title: "classification_models"
author: "Bolu Oluwalade"
date: "2022-09-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(xgboost)
library(e1071)
library(caret)
library(kernlab)
library(MLmetrics)
library(caretEnsemble)
library(gbm)

set.seed(123)
```


```{r}
df <- read_csv("jogging_walking.csv") |>
  janitor::clean_names() |> 
  select(-1)

#clasess
table(df$activity)
```

### Create test and train data
```{r}
data <- df[sample(nrow(df)),]
bound <- floor(0.8 * nrow(data))
df_train<- data[1:bound,]
df_test <- data[(bound+1):nrow(df),]
cat("number of train and test data are:",nrow(df_train),"and",nrow(df_test),"respectively")

```


```{r}
xtrain<- subset(df_train,select = -activity)
ytrain <- df_train$activity
xtest<- subset(df_test,select = -activity)
ytest <- factor(df_test$activity)
```

### Support vector machine model
```{r}
#reoeated cross validation sampling method
ctrl <- trainControl(
  method="repeatedcv",
  number = 5,
  repeats = 5
  ) 

svm_class = train(
  xtrain,
  ytrain,
  method = "svmLinear",
  trControl = ctrl
  )

print(svm_class)
```

### SVM confusion matrix
```{r}
svm.y_pred <-predict(
  svm_class,
  newdata = xtest
  )

confusionMatrix(svm.y_pred,factor(ytest))
```

### KNN Model
```{r}
set.seed(123)
ctrl <- trainControl(method="repeatedcv",number = 5,repeats = 5)
knn_model <- train(
  xtrain, ytrain,
  method = "knn",
  trControl = ctrl,
  tuneGrid = expand.grid(k=c(1,3,5,7,9))
  )
print(knn_model)
```

### KNN confusion method
```{r}
knn.y_pred <-predict(knn_model,newdata = xtest)
confusionMatrix(knn.y_pred,ytest)
```

### XGBoost 
```{r}
set.seed(123)

xgb_grid = expand.grid(
    nrounds = 1000,
    eta = c(0.01, 0.001, 0.0001),
    max_depth = c(2, 4, 6, 8, 10),
    gamma = 1,
    colsample_bytree = 0.8,min_child_weight=0,subsample =1
)

ctrl <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  returnData = FALSE,
  returnResamp = "all",  # save losses across all models
  classProbs = TRUE,# set to TRUE for AUC to be computed
  summaryFunction = twoClassSummary,
  allowParallel = TRUE,
)

xgb_model1 <- train(
  xtrain,
  ytrain,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid =xgb_grid
  )

print(xgb_model1)

```


```{r}
xgb1.y_pred <-predict(xgb_model1,xtest)
confusionMatrix(xgb1.y_pred,ytest)
plot(xgb_model1)
```


### Stacking models
```{r}

# create submodels
control <- trainControl(
  method="repeatedcv",
  number=5,
  repeats=3,
  savePredictions=TRUE,
  classProbs=TRUE
  )

algorithmList <- c('lda', 'rpart',"svmLinear",'xgbTree', 'knn', 'xgbLinear')

set.seed(123)
models <- caretList(
  activity~.,
  data = df_train,
  trControl=control,
  methodList=algorithmList
  )
results <- resamples(models)
summary(results)
dotplot(results)
```


```{r}
splom(results)
```


```{r}
# stack using random forest
set.seed(123)
stackControl <- trainControl(
  method="repeatedcv",
  number=10,
  repeats=3,
  savePredictions=TRUE,
  classProbs=TRUE
  )

stack.rf <- caretStack(models, method="rf", metric="Accuracy", trControl=stackControl)
print(stack.rf)
```

### Save model
```{r}
saveRDS(stack.rf,"stack.rf.rds")
```

